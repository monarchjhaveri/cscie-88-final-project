from scrapy import Spider
from scrapy.selector import Selector
from scrapy.http.request import Request

from scrapy.item import Item, Field
import os
import random

class StackItem(Item):
    title = Field()
    url = Field()

class StackSpider(Spider):
    name = "stack"
    allowed_domains = ["npmjs.com"]
#    start_urls = [
#          "https://www.npmjs.com/package/browserify",
#    ]

    def __init__(self, start_url):
        Spider.__init__(self)
        self.start_urls.append(start_url)
		
    def parse(self, response):

        siblings = Selector(response).xpath(".//h3[contains(text(),'Dependencies (')]/following-sibling::p")
        for sibling in siblings:
            attributes = sibling.xpath(".//a[contains(@href,'package')]/@href").extract()
            for attribute in attributes:
               item = StackItem()
               item['url'] = "https://www.npmjs.com" + attribute
               f = open('workfile' + str(random.randint(0, 9999999999)),'a+')
               if item['url'] not in f.read():
                  f.write(item['url'] + os.linesep)
                  f.close
                  yield item
                  yield Request("https://www.npmjs.com" + attribute, callback=self.parse)

